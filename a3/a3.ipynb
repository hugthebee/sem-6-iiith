{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**: Sreeja Guduri <br>\n",
    "**Assignment - 3**<br>\n",
    "**Roll number**: 2021102007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><u>Question - 1</h1></u>\n",
    "\n",
    "<h4><u>PART-1</u></h4>\n",
    "\n",
    "* For this question, we are expected to detect faces from the first 30 seconds of a scence from the movie 'Forrest Gump'.\n",
    "* 'ffmpeg' is used to extract the frames (assuming 24fps) from the first 30 seconds. This gives us about 720 frames.\n",
    "<br>\n",
    "\n",
    "The command used is: 'ffmpeg -i Forrest.mp4 -ss 00:00:00 -t 00:00:30 -vf fps=24 frames/frame_%04d.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><u>PART-2 & PART-3</u></h4>\n",
    "\n",
    "* The Viola Jones Harr Cascades face detector (from the OpenCV library) is used to detect faces. This algorithm uses Harr features, an integral image and a cascade classifier to determine if an object is a face or not.\n",
    "\n",
    "* The average processing time per frame is 0.031230 seconds. This means this algorithm is suitable for real time face detection.\n",
    "\n",
    "*  The main paramters that could change the processing time are:\n",
    "1. <u>scaleFactor:</u> specifies how much the image size is reduced at each image scale. If it is a large value, the processing time is less but we could miss the fine details of the object, making the detection less accurate. A smaller value means more processing time but higher accuracy.\n",
    "2. <u>minNeighbours:</u> specifies how many neighbouring pixels we look at when considering a feature. Higher the number of neighbours we need to look at, slower in the computation.\n",
    "3. The XML file used is: **haarcascade_frontalface_alt.xml**, which is a lot better than **haarcascade_frontalface_default.xml** because it detects a lot of false positives.\n",
    "\n",
    "* The video with the detected faces is then saved using the command: 'ffmpeg -i Forrest.mp4 -ss 00:00:00 -t 00:00:30 -vf fps=24 frames/frame_%04d.png'\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Video link: https://drive.google.com/file/d/1pY9bOBxm4xIQ2BV7aGYxC_GgN7y4xLGQ/view?usp=sharing\n",
    "\n",
    "Three cases when the detector works/fails:\n",
    "1. The Harr Casade Detector works only for frontal faces (faces directly facing the camera). Thus, parts of the video where they are facing to the side are not accurately detected. \n",
    "2. When the people in the scene are far away and their faces are small, the face detector doesn't work well either. This could be because the feature detector is big and thus missed smaller, finer details of the small faces. \n",
    "3. The face detector works the best when the faces are close to the camera, facing front and are well-lit and illuminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean processing time for each frame is 0.030082630780008106 seconds\n"
     ]
    }
   ],
   "source": [
    "face_cascade = cv.CascadeClassifier(cv.data.haarcascades + 'haarcascade_frontalface_alt.xml')\n",
    "mean = 0\n",
    "box_coord = []\n",
    "\n",
    "for i in range(1, 721):\n",
    "    frame = cv.imread(f'frames/frame_{i:04d}.png')\n",
    "    gray_frame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    start_time = time.time()\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=4, minSize=(30, 30))\n",
    "    end_time = time.time()\n",
    "    mean += end_time - start_time\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "    #save to a new folder\n",
    "    cv.imwrite(f'output_frames/frame_{i:04d}.png', frame)\n",
    "    box_coord.append(faces)\n",
    "    \n",
    "mean /= 720\n",
    "print(f'The mean processing time for each frame is {mean} seconds')\n",
    "cv.destroyAllWindows()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[143 213 122 122]]\n"
     ]
    }
   ],
   "source": [
    "print(box_coord[30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><u>PART-4</u></h4>\n",
    "\n",
    "* A naive face tracking algorithm is implemented to keep a track of the various faces that are detected.\n",
    "*  The idea is to compare the faces detected in two consecutive frames and give two faces the same ID if their Intersection over Union (IoU) is greater than 0.5.\n",
    "* If a new face is encountered, we give it a new ID and if a face doesn't appear in the next frame, we end its track.\n",
    "\n",
    "<br> \n",
    "\n",
    "A total of 57 unique tracks were created in the first 30 seconds. There were certain false positives that caused the number to go high also.\n",
    "\n",
    "<br>\n",
    "\n",
    "As is evident from the video - https://drive.google.com/file/d/1lEgDH2x8ZV3q289k01wv1Lht-RM24KKB/view?usp=sharing\n",
    " - the tracking is not very reliable. Faces are tracked with the same ID if the repv scene was similar and did not move a lot. \n",
    "* At 0:01, we see that the girls face is assosciated with both IDs 4 and 5 even though they should get the same ID. This is because girls face is not detected in the frame in between those two frames (possibly because she was facing sideways). Thus, the algorithm assumes ends the track and starts a new one when the face is detected again.\n",
    "\n",
    "* At 0:12 also there is a case of a false positive (the boys' ear is wrongly detected as a face) and so this throws off the whole face tracking process because it is assigned a unique face ID (eventhough it is not a face).\n",
    "\n",
    "* However different people never get assosciated with the same track. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(box1, box2):\n",
    "    x1_box1, y1_box1, x2_box1, y2_box1 = box1\n",
    "    x1_box2, y1_box2, x2_box2, y2_box2 = box2\n",
    "    \n",
    "    # Calculate intersection coordinates\n",
    "    x1_intersection = max(x1_box1, x1_box2)\n",
    "    y1_intersection = max(y1_box1, y1_box2)\n",
    "    x2_intersection = min(x2_box1, x2_box2)\n",
    "    y2_intersection = min(y2_box1, y2_box2)\n",
    "    \n",
    "\n",
    "    area = max(0, x2_intersection - x1_intersection) * max(0, y2_intersection - y1_intersection)\n",
    "    \n",
    "    \n",
    "    area_box1 = (x2_box1 - x1_box1) * (y2_box1 - y1_box1)\n",
    "    area_box2 = (x2_box2 - x1_box2) * (y2_box2 - y1_box2)\n",
    "    union_area = area_box1 + area_box2 - area\n",
    "    \n",
    "    # Calculate IoU score\n",
    "    if union_area > 0 :\n",
    "        iou = area / union_area\n",
    "    else: \n",
    "        iou = 0\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tracks created: 57\n"
     ]
    }
   ],
   "source": [
    "# boxcoord has the coordinates of the first 30 secs\n",
    "track_id = 0\n",
    "face_tracks = []\n",
    "\n",
    "for i in range(720):\n",
    "        face_match = []\n",
    "        \n",
    "        for (x,y,w,h) in box_coord[i]:\n",
    "                matched = False\n",
    "                for track in face_tracks:\n",
    "                        prev_frame,prev_id,prev_box = track\n",
    "                        if i - prev_frame==1:\n",
    "                                if iou(prev_box, (x,y,x+w,y+h)) > 0.5:\n",
    "                                        face_match.append((i,prev_id,(x,y,x+w,y+h)))\n",
    "                                        matched = True\n",
    "                                        break\n",
    "                if not matched:\n",
    "                        face_match.append((i,track_id,(x,y,x+w,y+h)))\n",
    "                        track_id += 1  \n",
    "                        \n",
    "        face_tracks.extend(face_match)      \n",
    "\n",
    "unique_tracks = set()\n",
    "for track in face_tracks:\n",
    "    frame_num, face_id, _ = track\n",
    "    unique_tracks.add(face_id)\n",
    "\n",
    "num_unique_tracks = len(unique_tracks)\n",
    "print(\"Number of unique tracks created:\", num_unique_tracks)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, track in enumerate(face_tracks):\n",
    "    frame_num, face_id, box = track\n",
    "    frame = cv.imread(f'output_frames/frame_{frame_num+1:04d}.png')\n",
    "    cv.putText(frame, f'ID: {face_id}', (box[0], box[1] - 10), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    cv.imwrite(f'output_frames/frame_{frame_num+1:04d}.png', frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><u>QUESTION-2</u></h1>\n",
    "\n",
    "The link to the google collab with the second question is here: https://colab.research.google.com/drive/1vDlGA4tDDnAGXs6cRdxAARZugZSWefrM?usp=sharing\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
